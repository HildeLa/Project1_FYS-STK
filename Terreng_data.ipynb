{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2617b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae5b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting R2 and Mean Square Error of model predictions\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model)**2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496012cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making feature matrix depending with complexity given by n\n",
    "def create_X(x, y, n):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x) \n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x) \n",
    "    l = int((n+1)*(n+2)/2)    \n",
    "    X = np.ones((N,l)) \n",
    "    \n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4949c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS \n",
    "def find_beta(X, z): \n",
    "    XT = X.T\n",
    "    XTXinv = np.linalg.pinv(np.matmul(XT, X))\n",
    "    XTz = np.matmul(XT, z)\n",
    "    beta = np.matmul(XTXinv, XTz)\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def standardscaler(X_train, X_test, z_train, z_test): \n",
    "    #Using Sci-kit learn standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    # Scaling independent variable\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    #scaling response variable\n",
    "    z_train_scaled = (z_train - np.mean(z_train))/np.std(z_train)\n",
    "    z_test_scaled = (z_test - np.mean(z_train))/np.std(z_train)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, z_train_scaled, z_test_scaled\n",
    "\n",
    "\n",
    "def meanscaler(X_train, X_test, z_train, z_test):\n",
    "    #Taking the mean of the data\n",
    "    mean_X = np.mean(X_train, axis=0)\n",
    "    mean_z = np.mean(z_train)\n",
    "    #Subtract mean from all data points\n",
    "    X_train_scaled = X_train - mean_X\n",
    "    X_test_scaled = X_test - mean_X\n",
    "    z_train_scaled = z_train - mean_z\n",
    "    z_test_scaled = z_test - mean_z\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, z_train_scaled, z_test_scaled\n",
    "\n",
    "    \n",
    "def scalerMinMax(X_train, X_test, z_train, z_test):\n",
    "    #Using Sci-kit learn min-max scaler\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(X_train)\n",
    "    z_test_scaled = (z_test - np.mean(z_train))/np.std(z_train)\n",
    "    #Scaling independent varaible\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    #reshape response varible to be scaled\n",
    "    z_train = z_train.reshape((-1,1))\n",
    "    z_test = z_test.reshape((-1,1))\n",
    "    #Scaling response variable\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(z_train)\n",
    "    z_train_scaled = scaler.transform(z_train)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1)).fit(z_train)\n",
    "    z_test_scaled = scaler.transform(z_test)\n",
    "    # reshaping\n",
    "    z_train_scaled.flatten()\n",
    "    z_test_scaled.flatten() \n",
    "    return X_train_scaled, X_test_scaled, z_train_scaled, z_test_scaled\n",
    "\n",
    "\n",
    "def robustscaler(X_train, X_test, z_train, z_test):\n",
    "     #Using Sci-kit learn robust scaler\n",
    "    scaler = RobustScaler().fit(X_train)\n",
    "    #scaling independent variable\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #Scaling response variable\n",
    "    z_train = z_train.reshape((-1,1))\n",
    "    z_test = z_test.reshape((-1,1))\n",
    "    scaler = RobustScaler().fit(z_train)\n",
    "    z_train_scaled = scaler.transform(z_train)\n",
    "    scaler = RobustScaler().fit(z_train)\n",
    "    z_test_scaled = scaler.transform(z_test)\n",
    "    z_train_scaled = z_train_scaled.flatten()\n",
    "    z_test_scaled = z_test_scaled.flatten()\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, z_train_scaled, z_test_scaled\n",
    "\n",
    "\n",
    "def nonscaler(X_train, X_test, z_train, z_test):\n",
    "    return(X_train, X_test, z_train, z_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping\n",
    "def bootstrap(x, z, x_test, z_test, iterations = 100):\n",
    "    MSEs = np.zeros(iterations) \n",
    "    R2s = np.zeros(iterations) \n",
    "    z_preds = np.zeros((len(z_test), iterations)) \n",
    "    for i in range(iterations):\n",
    "        bt_x, bt_z = resample(x, z)\n",
    "        beta = find_beta(bt_x, bt_z) #Finding beta with new x train and z train\n",
    "        z_pred = x_test @ beta #predict z with x_test\n",
    "        z_preds[:, i] = z_pred.ravel()\n",
    "        mse = MSE(z_test, z_pred)\n",
    "        r2 = R2(z_test, z_pred) # getting statistics of prediction in current bootstrap\n",
    "        MSEs[i] = mse\n",
    "        R2s[i] = r2\n",
    "    \n",
    "    zpreds = z_preds.ravel()\n",
    "    z_test = z_test.reshape((-1, 1))\n",
    "    bt_err = np.mean( np.mean((z_test - z_preds)**2, axis=1, keepdims=True))\n",
    "    bt_bias = np.mean((z_test - np.mean(z_preds, axis=1, keepdims=True))**2)\n",
    "    bt_var = np.mean( np.var(z_preds, axis=1, keepdims=True) )\n",
    "    #bt_var = np.mean( np.var(z_preds) )\n",
    "    boot_MSE = np.mean(MSEs)\n",
    "    boot_R2 = np.mean(R2s)\n",
    "    \n",
    "    return boot_MSE, boot_R2, bt_err, bt_bias, bt_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13375b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation\n",
    "\n",
    "def cross_validation(feature, data, kfolds):\n",
    "    n = len(data)\n",
    "    #samme som tidligere, bare her uten replacement\n",
    "    indices = np.random.choice(n, n, replacement=False)\n",
    "    scores = np.zeros(kfolds)\n",
    "    \n",
    "    #definerer dimensjonene og størrelsene på de ulike delene\n",
    "    fold_len = int(n/kfolds)\n",
    "    feature_train = np.zeros([(kfolds-1)*fold_len, len(feature[0])])\n",
    "    data_train = np.zeros((tkfolds-1)*fold_len)\n",
    "    feature_test = np.zeros([fold_len, len(feature[0])])\n",
    "    data_test = np.zeros(fold_len)\n",
    "    feature_best = np.copy(feature_test)\n",
    "    data_best = np.copy(data_train)\n",
    "    data_test_best = np.copy(data_test)\n",
    "    for i in range(kfolds):\n",
    "        #De første fire delene sine indekser\n",
    "        fold_indices = indices[:(kfolds-1)*fold_len]\n",
    "        \n",
    "        feature_train = feature[fold_indices]\n",
    "        data_train = data[fold_indices]\n",
    "        \n",
    "        #den siste delens indekser\n",
    "        test_fold_indices = indices[(kfolds-1)*fold_len:]\n",
    "        \n",
    "        feature_test = feature[test_fold_indices]\n",
    "        data_test = data[test_fold_indices]\n",
    "        \n",
    "        #finner scorene\n",
    "        beta = find_beta(feature_train, data_train)\n",
    "        data_ = feature_test @ beta\n",
    "        scores[i] = MSE(data_test, data_)\n",
    "        #om i>0 er det automatisk den beste modellen så langt\n",
    "        #if test nr.2 fungerer også ikke for i=0\n",
    "        #den nye beste modellen blir lagret for senere bruk\n",
    "        if i > 0:\n",
    "            #sjekker om den nye scoren er bedre\n",
    "            if scores[i] < scores[:i].min():\n",
    "                feature_best = feature_test\n",
    "                data_best = data_\n",
    "                data_test_best = data_test\n",
    "        else:\n",
    "            feature_best = feature_test\n",
    "            data_best = data_\n",
    "            data_test_best = data_test\n",
    "        \n",
    "        #ruller indeksene til høyre, slik at den tidligere siste delen er nå først\n",
    "        indices = np.roll(indices, fold_len)\n",
    "        \n",
    "    return scores, data_best, feature_best, data_test_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
